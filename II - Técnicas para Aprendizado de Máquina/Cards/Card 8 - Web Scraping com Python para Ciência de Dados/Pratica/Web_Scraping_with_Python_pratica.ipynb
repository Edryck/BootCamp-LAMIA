{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a290fb76-e0fa-42bb-87d2-89c931af18b6",
   "metadata": {},
   "source": [
    "# Web Scraping com Python - Prática"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbac04c-6f3a-450a-907a-979e9ecb2010",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc5e6f6-80b9-475d-a17d-280b99b8be34",
   "metadata": {},
   "source": [
    "O objetivo desta aula foi aprender <i>Web Scraping</i> utilizando a linguagem Python e a biblioteca <i>BeautifulSoup</i> e <i>Requests</i>. Após assitir a aula criei um programa bem simples para extração de dados de um site de animes, onde eu pegar o nome do anime, o último episódio lançado e o link do mesmo e armazenava em um arquivo CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca631d2-77e5-4e2c-a32f-44067063e150",
   "metadata": {},
   "source": [
    "## Desenvolvimento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a21c8ff-8038-418f-b874-00ddea14048e",
   "metadata": {},
   "source": [
    "Inicialmente eu comecei testando o básico que envolvia conseguir o HTML do site, então importei as bibliotecas e criei uma instância da biblioteca <i>BeautifulSoup</i> e sei a função `get()`da biblioteca <i>Requests</i> para conseguir o HTML do site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b561468-72bc-4289-86d2-42d01b2af148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "site_url='https://animesonlinecc.to/episodio/'\n",
    "\n",
    "html = requests.get(site_url).text\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fbf2cf-9244-40d3-a71d-83dfab8c2be3",
   "metadata": {},
   "source": [
    "Em seguida comecei procurando as <i>tags</i> da parte onde está os dados de cada animes.  \n",
    "Usei a ferramenta Inspencionar do navegador e encontrei as seguintes informações:  \n",
    "- Cada episódio estava em uma <i>tag</i> do tipo <i>article</i>.\n",
    "- Dentro desta <i>tag</i> tinha duas <i>div</i>, uma que tinha a imagem e a outra com o texto com as informações que eu precisava (nome e episódio) e o <i>link</i>.\n",
    "- As classes de cada um (<i>article, div</i>).\n",
    "  \n",
    "Agora que tinha o que eu precisava continuei com o código.\n",
    "Na parte de conseguir o <i>link</i> foi tranquio, já o nome e episódio teve que fazer outra coisa. O nome e episódio do anime vinha junto então inicialmente considerei como dados brutos (o nome da variável \"`raw_data`\") e depois separei de uma forma que só dá certo de episódios de 0 a 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a17004-2ebb-4dd3-a067-eab9057320e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yuusha Party wo Oidasareta Kiyoubinbou Episodio 7\n",
      " Oshi no Ko 3 Episodio 5\n",
      " Okiraku Ryoushu no Tanoshii Ryouchi Bouei Episodio 6\n",
      " 29-sai Dokushin Chuuken Boukensha no Nichijou Episodio 6\n",
      " Darwin Jihen Episodio 6\n",
      " Yuusha Party ni Kawaii Ko ga Ita no de, Kokuhaku Shitemita Episodio 6\n",
      " Isekai no Sata wa Shachiku Shidai Episodio 6\n",
      " Mayonaka Heart Tune Episodio 6\n",
      " Maou no Musume wa Yasashi Sugiru!! Episodio 7\n",
      " Osananajimi to wa Love Comedy ni Naranai Episodio 6\n",
      " Golden Kamuy: Saishuushou Episodio 6\n",
      " Vigilante: Boku no Hero Academia Illegals 2 Episodio 6\n",
      " Kirei ni Shitemoraemasu ka Episodio 6\n",
      " Jigokuraku 2 Episodio 5\n",
      " Kizoku Tensei: Megumareta Umare kara Saikyou no Chikara wo Eru Episodio 6\n",
      " Akuyaku Reijou wa Ringoku no Outaishi ni Dekiai sareru Episodio 5\n",
      " Hanazakari no Kimitachi e Episodio 7\n",
      " Majutsushi Kunon wa Mieteiru Episodio 7\n",
      " Seihantai na Kimi to Boku Episodio 5\n",
      " Kaya-chan wa Kowakunai Episodio 5\n"
     ]
    }
   ],
   "source": [
    "# Todos os animes da página\n",
    "animes = soup.find_all('article', class_='item se episodes')\n",
    "\n",
    "# Dentro de um loop para pegar todos os encontrados na página\n",
    "for anime in animes:\n",
    "    # Dados brutos\n",
    "    raw_data = anime.find('div', class_='eptitle').text\n",
    "    print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba1e06e-ca8f-4852-a3d2-a0055e067e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yuusha Party wo Oidasareta Kiyoubinbou\n",
      "Episodio 7\n",
      "https://animesonlinecc.to/episodio/yuusha-party-wo-oidasareta-kiyoubinbou-episodio-7/\n",
      "\n",
      "Oshi no Ko 3\n",
      "Episodio 5\n",
      "https://animesonlinecc.to/episodio/oshi-no-ko-3-episodio-5/\n",
      "\n",
      "Okiraku Ryoushu no Tanoshii Ryouchi Bouei\n",
      "Episodio 6\n",
      "https://animesonlinecc.to/episodio/okiraku-ryoushu-no-tanoshii-ryouchi-bouei-episodio-6/\n",
      "\n",
      "29-sai Dokushin Chuuken Boukensha no Nichijou\n",
      "Episodio 6\n",
      "https://animesonlinecc.to/episodio/29-sai-dokushin-chuuken-boukensha-no-nichijou-episodio-6/\n",
      "\n",
      "Darwin Jihen\n",
      "Episodio 6\n",
      "https://animesonlinecc.to/episodio/darwin-jihen-episodio-6/\n",
      "\n",
      "Yuusha Party ni Kawaii Ko ga Ita no de, Kokuhaku Shitemita\n",
      "Episodio 6\n",
      "https://animesonlinecc.to/episodio/yuusha-party-ni-kawaii-ko-ga-ita-no-de-kokuhaku-shitemita-episodio-6/\n",
      "\n",
      "Isekai no Sata wa Shachiku Shidai\n",
      "Episodio 6\n",
      "https://animesonlinecc.to/episodio/isekai-no-sata-wa-shachiku-shidai-episodio-6/\n",
      "\n",
      "Mayonaka Heart Tune\n",
      "Episodio 6\n",
      "https://animesonlinecc.to/episodio/mayonaka-heart-tune-episodio-6/\n",
      "\n",
      "Maou no Musume wa Yasashi Sugiru!!\n",
      "Episodio 7\n",
      "https://animesonlinecc.to/episodio/maou-no-musume-wa-yasashi-sugiru-episodio-7/\n",
      "\n",
      "Osananajimi to wa Love Comedy ni Naranai\n",
      "Episodio 6\n",
      "https://animesonlinecc.to/episodio/osananajimi-to-wa-love-comedy-ni-naranai-episodio-6/\n",
      "\n",
      "Golden Kamuy: Saishuushou\n",
      "Episodio 6\n",
      "https://animesonlinecc.to/episodio/golden-kamuy-saishuushou-episodio-6/\n",
      "\n",
      "Vigilante: Boku no Hero Academia Illegals 2\n",
      "Episodio 6\n",
      "https://animesonlinecc.to/episodio/vigilante-boku-no-hero-academia-illegals-2-episodio-6/\n",
      "\n",
      "Kirei ni Shitemoraemasu ka\n",
      "Episodio 6\n",
      "https://animesonlinecc.to/episodio/kirei-ni-shitemoraemasu-ka-episodio-6/\n",
      "\n",
      "Jigokuraku 2\n",
      "Episodio 5\n",
      "https://animesonlinecc.to/episodio/jigokuraku-2-episodio-5/\n",
      "\n",
      "Kizoku Tensei: Megumareta Umare kara Saikyou no Chikara wo Eru\n",
      "Episodio 6\n",
      "https://animesonlinecc.to/episodio/kizoku-tensei-megumareta-umare-kara-saikyou-no-chikara-wo-eru-episodio-6/\n",
      "\n",
      "Akuyaku Reijou wa Ringoku no Outaishi ni Dekiai sareru\n",
      "Episodio 5\n",
      "https://animesonlinecc.to/episodio/akuyaku-reijou-wa-ringoku-no-outaishi-ni-dekiai-sareru-episodio-5/\n",
      "\n",
      "Hanazakari no Kimitachi e\n",
      "Episodio 7\n",
      "https://animesonlinecc.to/episodio/hanazakari-no-kimitachi-e-episodio-7/\n",
      "\n",
      "Majutsushi Kunon wa Mieteiru\n",
      "Episodio 7\n",
      "https://animesonlinecc.to/episodio/majutsushi-kunon-wa-mieteiru-episodio-7/\n",
      "\n",
      "Seihantai na Kimi to Boku\n",
      "Episodio 5\n",
      "https://animesonlinecc.to/episodio/seihantai-na-kimi-to-boku-episodio-5/\n",
      "\n",
      "Kaya-chan wa Kowakunai\n",
      "Episodio 5\n",
      "https://animesonlinecc.to/episodio/kaya-chan-wa-kowakunai-episodio-5/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agora a separação do titulo do episodio\n",
    "for anime in animes:\n",
    "    raw_data = anime.find('div', class_='eptitle').text\n",
    "    anime_name = raw_data[1:-11].strip()\n",
    "    anime_episode = raw_data[-11:].strip()\n",
    "    # Pegando o link do episódio\n",
    "    anime_link = anime.find('a')['href']\n",
    "\n",
    "    print(anime_name)\n",
    "    print(anime_episode)\n",
    "    print(anime_link)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44bd335-b93c-45ce-bf11-7d85b343db4e",
   "metadata": {},
   "source": [
    "Agora que temos o básico pronto, eu pensei em pegar todos eles e armazenar em algum local, como já mexi com Pandas pensei em armazenar em um arquivo CSV, assim poderia transformar em um <i>Dataframe</i> depois (como é um CSV, poderia ver no Excel também).  \n",
    "Para isso eu precisei importar a biblioteca <i>csv</i> e agora comecei as fazer a manipulação de arquivo me Python, eu não poderia pagar o arquivo e reescrever porque eu iria perder os dados salvos, também não poderia ficar escrevendo o mesmo anime com a mudança de um episódio e o <i>link</i>, ficaria muita coisa repetida (tendo em vista que eu só queria ter o nome e o último episódio lançado).  \n",
    "Sendo assim, pensei em apenas fazer três condições:  \n",
    "- Se já estiver o no CSV e lançou um episódio, reescreve o <i>link</i> e o episódio.\n",
    "- Se não estiver no CSV, adiciona no fim nome, episódio e <i>link</i>.\n",
    "- Se não caiu em nenhumas das duas de cima, não altera nada.\n",
    "\n",
    "Criei duas funções, uma para carregar o arquivo e outra para salvar o arquivo com os dados atualizados (`load_data(file_path)` e `save_data(file_path, data)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a20407e7-ac17-4812-887f-2260f24f9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file='animes_pratica.csv'\n",
    "\n",
    "# Função para carregar os dados do arquivo\n",
    "def load_data(file_path):\n",
    "    # Os dados são adicionados em um Dictinary\n",
    "    data={}\n",
    "    # Abre o arquivo para leitura, o encoding é para a leitura correta de caractere com acentuação\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for line in r:\n",
    "            # Para cada linha, ele vai ler o nome que está em data e atribuir o ultimo episodio salvo e o link dele\n",
    "            data[line['Nome']] = [line['Ultimo_Episodio'], line['Link']]\n",
    "    return data\n",
    "\n",
    "# Salvar os dados no arquivo csv\n",
    "def save_data(file_path, data):\n",
    "    # Vai abrir para escrita\n",
    "    with open(file_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        w = csv.writer(f)\n",
    "        # Escreve a o cabeçalho para o DictReader funcionar\n",
    "        w.writerow(['Nome', 'Ultimo_Episodio', 'Link'])\n",
    "        for name, info in data.items():\n",
    "            # Escreve a linha, info[0] é o último episódio e o info[1] é  o link\n",
    "            w.writerow([name, info[0], info[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836fe78-d028-4dac-873b-6b9f8dadf063",
   "metadata": {},
   "source": [
    "Agora juntando tudo, também adicionando para atualizar a leitura e coleta a cada uma hora (necessário a biblioteca <i>time</i>), finalmente termina esse programa simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacabb3b-91ea-4638-9e2f-12549db5df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "\n",
    "site_url='https://animesonlinecc.to/episodio/'\n",
    "csv_file = 'animes_pratica.csv'\n",
    "\n",
    "# Carrega os dados do arquivo csv\n",
    "def load_data(file_path):\n",
    "    data={}\n",
    "    # Abre o arquivo para leitura, o encoding é para a leitura correta de caractere com acentuação\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for line in r:\n",
    "            # Para cada linha, ele vai ler o nome que está em data e atribuir o ultimo episodio salvo e o link dele\n",
    "            data[line['Nome']] = [line['Ultimo_Episodio'], line['Link']]\n",
    "    return data\n",
    "\n",
    "# Salvar os dados no arquivo csv\n",
    "def save_data(file_path, data):\n",
    "    # Vai abrir para escrita\n",
    "    with open(file_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        w = csv.writer(f)\n",
    "        # Escreve a o cabeçalho para o DictReader funcionar\n",
    "        w.writerow(['Nome', 'Ultimo_Episodio', 'Link'])\n",
    "        for name, info in data.items():\n",
    "            # Escreve a linha, info[0] é o último episódio e o info[1] é  o link\n",
    "            w.writerow([name, info[0], info[1]])\n",
    "\n",
    "# O básico da extração coloquei dentro de uma função para posteriormente ser chamada na main\n",
    "def find_new_episodes(site_url):\n",
    "    # Tempo de espera para atualizar e ver se tem novos episodios\n",
    "    wait_time = 60 * 60 # 1 hora\n",
    "\n",
    "    while True:\n",
    "        # Carrega os dados do arquivo\n",
    "        my_animes = load_data(csv_file)\n",
    "        # Para ver se houve alguma mudança, nesse caso seria se teve novos episodios lançados\n",
    "        change = False\n",
    "        print('\\tÚltimos episódios lançados\\n\\n')\n",
    "\n",
    "        html = requests.get(site_url).text\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        animes = soup.find_all('article', class_='item se episodes')\n",
    "        \n",
    "        for anime in animes:\n",
    "            # Aqui ele vai pegar os dados brutos (brutos por que vem assim: \"<nome-do-anime> <episodio>\")\n",
    "            raw_data = anime.find('div', class_='eptitle').text\n",
    "            anime_name = raw_data[1:-11].strip()\n",
    "            anime_episodio = raw_data[-11:].strip()\n",
    "            # Pega o link para o episódio\n",
    "            anime_link = anime.find('a')['href']\n",
    "            \n",
    "            # Para verificar se o anime já está na lista\n",
    "            if anime_name in my_animes:\n",
    "                # Se estiver na lista, ele vai verificar se o último episódio é igual o que está no arquivo\n",
    "                if my_animes[anime_name][0] != anime_episodio:\n",
    "                    print(f\"Atualizando: {anime_name} ({my_animes[anime_name][0]} -> {anime_episodio})\")\n",
    "                    my_animes[anime_name] = [anime_episodio, anime_link]\n",
    "                    # Se atualizou o episodio... Então houve mudança\n",
    "                    change = True\n",
    "            # Se não está na lista, acredito que seja um anime novo, como pega só da primeira página\n",
    "            # então pode ocorrer de pegar algo como: \"<nome> Episodio 8\", porque esse animes talvez não\n",
    "            # estava na listado na página durante a primeira coleta, e caso o anime já foi finalizado, não\n",
    "            # vai estar nessa lista (já que não vai ter novos episódios lançados\n",
    "            else:\n",
    "                print(f\"Novo anime encontrado: {anime_name}\")\n",
    "                my_animes[anime_name] = [anime_episodio, anime_link]\n",
    "                # Novo na lista, então houve mudança\n",
    "                change = True\n",
    "\n",
    "        # Se houve mudança no arquivo, ele salva\n",
    "        if change:\n",
    "            save_data(csv_file, my_animes)\n",
    "            print('\\nArquivo atualizado')\n",
    "        # Se não houve mudanças, não precisa salvar\n",
    "        else:\n",
    "            print('\\nNada de novo')\n",
    "\n",
    "        # Ele atualiza a cada uma hora\n",
    "        print(f\"Atualizando em {wait_time/60} minutos...\\n\")\n",
    "        time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a8bf2c-5c69-4d4a-becf-cec93ac67c6d",
   "metadata": {},
   "source": [
    "E por último, só precisa chamar a função principal `find_new_episodes(site_url)` na <i>main</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150abb2-2de5-4185-b624-a21e8104dc88",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "    find_new_episodes(site_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3e9a5-31f3-4859-8ac2-a52cba13a443",
   "metadata": {},
   "source": [
    "## Conclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fac478-edc0-44b6-ac84-70560ecb7d42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
