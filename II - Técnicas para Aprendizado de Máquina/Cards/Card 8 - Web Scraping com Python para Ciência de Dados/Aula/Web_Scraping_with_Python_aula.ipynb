{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b4126e-133f-4595-bb4f-ac6481ba7c2e",
   "metadata": {},
   "source": [
    "# Web Scraping com Python  \n",
    "Aqui será abordado alguns conceitos importantes sobre coleta de dados *web*, que é uma forma de mineração de dados que permite a extração de dados de *sites* da *web* convertendo-os em informações estruturada para posterior análise.  \n",
    "O tipo mais básico de coleta é download manual das páginas, copiando e colando o conteúdo, e isso é fundamental em Ciência de Dados.  \n",
    "Estaremos usando a biblioteca *BeautifulSoup*, esta biblioteca permite coletar qualquer informação de qualquer *site*, pode sua conta bancária, *site* de vagas de emprego como o LinkedIn, a Wikipédia, um *site* de esportes, enfim, qualquer coisa (ou melhor, *site*) que imaginar.\n",
    "\n",
    "<small>\n",
    "    O vídeo usado para a produção deste documento é <i>Web Scraping with Python - Beautiful Soup Crash Course</i> do canal <i>freeCodeCamp.org</i>.\n",
    "    \n",
    "    Link do vídeo: https://www.youtube.com/watch?v=XVv6mJpFOb0\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd245d0-68b5-4dcf-9d7c-13fb37499a2f",
   "metadata": {},
   "source": [
    "## Uso de Web Scraping para Arquivos Locais  \n",
    "O essencial para utilizar em arquivos locais é saber sobre manipulação de arquivos, no exemplo abaixo é mostrado abrindo um arquivo no modo de leitura e armazenando o conteúdo em uma variável `content`, para ver se deu certo é só imprimir a variável (`print(content)`). Agora utilizando a biblioteca <i>BeautifulSoup</i>, ela vai utilizada para formatar o HTML e trabalhar com as <i>tags</i> do HTML como objetos do Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d45476a6-21e9-4c61-8841-0781fc1bb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Abrir o arquivo\n",
    "with open('home_aula.html', 'r') as html_file:\n",
    "    # Lê o arquivo HTML e armazena na variável content\n",
    "    content = html_file.read()\n",
    "\n",
    "    # Intância do BeautifulSoup\n",
    "    # Como argumentos serão passados o arquivo HTML (content) que é o que eu quero extrair\n",
    "    # O segundo argumento é o método de análise sintática que vai usar\n",
    "    soup = BeautifulSoup(content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a3498d-51d2-4041-9f02-629e424e69c9",
   "metadata": {},
   "source": [
    "Imprimindo a variável `soup` agora ele vai mostrar toda a estrutura do HTML.  \n",
    "\n",
    "<small>**OBS.:** Se utilizar o método `prettify()`, ele vai imprimir de uma forma mais agradável, que seria tabulado.  \n",
    "A linha de código: `print(soup.prettify())`</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd9802-eb92-4379-a5f1-f9d7c317ae70",
   "metadata": {},
   "source": [
    "### Métodos `find()` e `find_all()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4125fe-5374-4450-ac47-863211e11d55",
   "metadata": {},
   "source": [
    "Supondo que queiramos obter todas as <i>tags</i> h5 (são <i>tags</i> de cabeçalho), vamos criar uma variável e atribuir a variável `soup` com o método `find()`, passando como parâmetro a <i>tag</i> h5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e0bf4fa-9eee-4cee-b1e6-0c2a7af72576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h5 class=\"card-title\">Python for beginners</h5>\n"
     ]
    }
   ],
   "source": [
    "# Variável que vai armazenar o h5 que queremos conseguir\n",
    "tags = soup.find('h5')\n",
    "# Imprimindo...\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cef715-6ccb-45f9-af02-5182075664ab",
   "metadata": {},
   "source": [
    "Como pode ver, a saída foi a primeira <i>tag</i> h5 encontrada, note que imprimiu apenas a primeira, caso queira encontrar todas, utilizamos o método `find_all()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b04a71-1a10-4914-9927-771c32db587f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h5 class=\"card-title\">Python for beginners</h5>, <h5 class=\"card-title\">Python Web Development</h5>, <h5 class=\"card-title\">Python Machine Learning</h5>]\n"
     ]
    }
   ],
   "source": [
    "tags = soup.find_all('h5')\n",
    "# Imprimindo...\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eab672-4372-46f4-8dff-61632932d901",
   "metadata": {},
   "source": [
    "A saída é uma lista com todas as <i>tags</i> h5, vamos tentar pegar apenas os nomes dentro dessas <i>tags</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13bdf300-0ff4-413c-930d-6934ca2b5b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python for beginners\n",
      "Python Web Development\n",
      "Python Machine Learning\n"
     ]
    }
   ],
   "source": [
    "for course in tags:\n",
    "    print(course.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ce86c-f7db-49df-a8a3-305adf988390",
   "metadata": {},
   "source": [
    "Assim a saída é todos os texto dentro das <i>tags</i> h5.  \n",
    "Isso pode ser um bom ponto de partida para entender como extrair informações de uma página web para conseguir dados específicos.  \n",
    "\n",
    "O `find_all()` também pode ser utilizado aplicando \"filtros\", por exemplo, tentar pegar o nome do curso e seu preço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91e5f918-a921-43e0-a024-5f446d277e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python for beginners\n",
      "Start for 20$\n",
      "Python Web Development\n",
      "Start for 50$\n",
      "Python Machine Learning\n",
      "Start for 100$\n"
     ]
    }
   ],
   "source": [
    "# Vai pegar todos as div que pertence a classe card\n",
    "course_cards = soup.find_all('div', class_='card')\n",
    "for course in course_cards:\n",
    "    # Pega o texto do cabeçalho do cord\n",
    "    course_name = course.h5.text\n",
    "    # Pega a tag onde está o texto do preço (ela está na tag <a>)\n",
    "    course_price = course.a\n",
    "\n",
    "    print(course_name)\n",
    "    # OBS.: Imprimir apenas a variável, vai mostrar todo o conteúdo da tag\n",
    "    # print(course_price)\n",
    "    # Saída: <a class=\"btn btn-primary\" href=\"#\">Start for 20$</a>\n",
    "    # Para imprimir apenas o texto, só utilizar o atributo text\n",
    "    course_price = course.a.text\n",
    "    print(course_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a80608-252f-4d0f-8274-d9ce323515e5",
   "metadata": {},
   "source": [
    "Para imprimir uma frase como \"Pytho for beginners costs 20$\", é só utilizar o método `spilt()`, ele irá acesar o último elemento do texto (visto que o preço é a última palavra).  \n",
    "Utilizaremos o `split()` e dividir pelo número de espaços em branco para não precisar especificar nada, como queremos o último elemento, vamos procurar pelo índice -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17206568-516f-4e6f-a9b1-621a055c4573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python for beginners costs 20$\n",
      "Python Web Development costs 50$\n",
      "Python Machine Learning costs 100$\n"
     ]
    }
   ],
   "source": [
    "for course in course_cards:\n",
    "    course_name = course.h5.text\n",
    "    course_price = course.a.text.split()[-1] \n",
    "    # Agora para imprimir como uma frase\n",
    "    print(f'{course_name} costs {course_price}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde52429-0557-41d1-9bc5-f45ed158f7f3",
   "metadata": {},
   "source": [
    "É algo bem interressante de se usar em sites reais como a Udemy, onde os preços dos cursos são atualizados constantemente, uma boa ideia seria executar esse programa a cada determinado periodo de tempo, por exemplo, semanalmente. Assim podendo ter uma capacidade de estar ciente de cada curso que a Udemy atualizou na página web onde estariamos extraindo esses dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13bafd4-baf6-4cd5-a2f7-2c5b4835ae3a",
   "metadata": {},
   "source": [
    "## Inspetor  \n",
    "O Inspetor do navegador é uma boa ferramenta para pode inspencionar o código das páginas, por exemplo, encontrar a tag onde está um botão como usamos no exemplo anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bc4f2c-3725-47eb-aa4f-7ef6766826cc",
   "metadata": {},
   "source": [
    "## Extraindo Dados de Sites Reais  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014b734-880c-4486-bf13-7fd916d16300",
   "metadata": {},
   "source": [
    "Agora vamos subir o nível e extrair dados de sites reais com a biblioteca Requests. O exemplo será feito em um site que busca anúncios de emprego e vamos trazer todos os anúncios de um site específico. Nesse exemplo, o programa irá extrair os anúncios de emprego mais recentes publicados nesse site."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7a144-858c-49b9-b9d2-476b4b928724",
   "metadata": {},
   "source": [
    "O principal passo aqui é garantir que a biblioteca Requests esteja instalada (no terminal: `pip install requests`). \n",
    "\n",
    "<small>**OBS.:** Como estou fazendo em um notebook não seria necessário importar o BeautifulSoup novamente, mas vou fazer isso para deixar junto das divisões (dividi em extração em arquivo e extração em site real).</samll>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b7c6c-c761-4cc4-a81f-e0b1f4cec212",
   "metadata": {},
   "source": [
    "### Requests  \n",
    "\n",
    "A biblioteca Requests faz nos bastidores é solicitar informações de um site específico. Seria algo como uma pessoa real acessando um site e solicitando algumas informações, inicialmente iremos utilizar o método `get()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0283984-c888-4729-b3f8-f36daccb6586",
   "metadata": {},
   "source": [
    "<small>**OBS.:** O HTML do site mudou, então nesse exemplo abaixo irei copiar o código da aula, mas ele não irá rodar.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ffaa17-3fee-4729-a4a9-59485e635902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edryck\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.timesjobs.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\Edryck\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.timesjobs.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Para usar o get, iremos passar a URL do site que vamos realizar a extração\n",
    "html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=e', verify=False).text\n",
    "# Criando uma instância do BS\n",
    "soup = BeautifulSoup(html_text, 'lxml')\n",
    "# Se utilizar o Inspencionar na página para localizar a parte que contém o conteúdo que queremos\n",
    "jobs = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')\n",
    "for job in jobs:\n",
    "    # Agora selecionando as vagas com a data de publicação especifica\n",
    "    # O texto dela vai estar dentro de um outro span\n",
    "    published_date = job.find('span', class_='sim-posted').span.text \n",
    "    if 'few' in published_date:\n",
    "        # Pegando agora o nome da empresa, na aula, a saída imprimindo apenas o texto, ele vem com alguns espaços\n",
    "        # Então será utilizado o replace() para substituir os espaços por nada\n",
    "        company_name = job.find('h3', class_='joblist-comp-name').text.replace(' ', '')\n",
    "        # Pegando as skills que a vaga pede\n",
    "        skills = job.find('span', class_='srp_skills').text.replace(' ', '')\n",
    "    \n",
    "        # Exibindo os dados da vaga\n",
    "        print(f'''\n",
    "        Company Name: {company_name}\n",
    "        Required Skills: {skills}\n",
    "        ''')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb6dd0b-d385-4dd3-92a5-964d948c8796",
   "metadata": {},
   "source": [
    "Basicamente é esse o código, ele vai pegar do site <i>TimesJobs</i> todas as vagas, eles estão em uma lista (tag li) e todas as que tiver a classe especificada no `soup.find_all()`vão ser armazenadas em `job`.  \n",
    "Em seguida, vai pegar as datas de publicações e passar pela condição, só vai extrair o nome da empresa e habilidades necessárias se foi publicadas a poucos dias.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369ce074-1ff9-4070-b30d-4de34fd1668d",
   "metadata": {},
   "source": [
    "## Formatação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c4eef-c582-4161-b414-d271a209dee7",
   "metadata": {},
   "source": [
    "A parte de extração foi realizada, agora vamos deixar mais agradável para vista, vamos formatar para deixar cada vaga com link para mais informações sobre ela e organizar.  \n",
    "<small>**OBS.:** Vou copiar o código anterior e aplicar as mudanças.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f619e-98b9-4d96-a713-6d808b497a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edryck\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.timesjobs.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\Edryck\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.timesjobs.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Para usar o get, iremos passar a URL do site que vamos realizar a extração\n",
    "html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=e', verify=False).text\n",
    "# Criando uma instância do BS\n",
    "soup = BeautifulSoup(html_text, 'lxml')\n",
    "# Se utilizar o Inspencionar na página para localizar a parte que contém o conteúdo que queremos\n",
    "jobs = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')\n",
    "for job in jobs:\n",
    "    # Agora selecionando as vagas com a data de publicação especifica\n",
    "    # O texto dela vai estar dentro de um outro span\n",
    "    published_date = job.find('span', class_='sim-posted').span.text \n",
    "    if 'few' in published_date:\n",
    "        # Pegando agora o nome da empresa, na aula, a saída imprimindo apenas o texto, ele vem com alguns espaços\n",
    "        # Então será utilizado o replace() para substituir os espaços por nada\n",
    "        company_name = job.find('h3', class_='joblist-comp-name').text.replace(' ', '')\n",
    "        # Pegando as skills que a vaga pede\n",
    "        skills = job.find('span', class_='srp_skills').text.replace(' ', '')\n",
    "        # O conteúdo onde está o link para mais informações da vaga\n",
    "        more_info = job.header.h2.a['href']\n",
    "        # Retirando o print com aspas triplas e substituindo pelas aspas duplas\n",
    "        # Vamos remover também os espaços em branco usando strip()\n",
    "        print(f\"Company Name: {company_name.strip()}\")\n",
    "        print(f\"Required Skills: {skills.strip()}\")\n",
    "        print(f\"More Info: {more_info}\")\n",
    "\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab6298-70df-47e9-9d28-6faf182105b6",
   "metadata": {},
   "source": [
    "## Filtro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63edb6-201d-4bfc-9d74-0f8c95362c58",
   "metadata": {},
   "source": [
    "Agora  vamos dar ao usuário a oprtunidade de filtrar alguns requisitos de habilidades que ele não possui.  \n",
    "<small>**OBS.:** Vou copiar o código anterior e aplicar as mudanças.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e94e09d-6a3c-4fc5-bae6-9687fcd3389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "print('Put some skill that you are not familiar with')\n",
    "# Variável para  o input usada no filtro\n",
    "unfamiliar_skill = input('>')\n",
    "print(f'Filtering out {unfamiliar_skill}')\n",
    "\n",
    "# Para usar o get, iremos passar a URL do site que vamos realizar a extração\n",
    "html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=e', verify=False).text\n",
    "# Criando uma instância do BS\n",
    "soup = BeautifulSoup(html_text, 'lxml')\n",
    "# Se utilizar o Inspencionar na página para localizar a parte que contém o conteúdo que queremos\n",
    "jobs = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')\n",
    "for job in jobs:\n",
    "    # Agora selecionando as vagas com a data de publicação especifica\n",
    "    # O texto dela vai estar dentro de um outro span\n",
    "    published_date = job.find('span', class_='sim-posted').span.text \n",
    "    if 'few' in published_date:\n",
    "        # Pegando agora o nome da empresa, na aula, a saída imprimindo apenas o texto, ele vem com alguns espaços\n",
    "        # Então será utilizado o replace() para substituir os espaços por nada\n",
    "        company_name = job.find('h3', class_='joblist-comp-name').text.replace(' ', '')\n",
    "        # Pegando as skills que a vaga pede\n",
    "        skills = job.find('span', class_='srp_skills').text.replace(' ', '')\n",
    "        # O conteúdo onde está o link para mais informações da vaga\n",
    "        more_info = job.header.h2.a['href']\n",
    "        # Filtro\n",
    "        if unfamiliar_skill not in skills:    \n",
    "            # Retirando o print com aspas triplas e substituindo pelas aspas duplas\n",
    "            # Vamos remover também os espaços em branco usando strip()\n",
    "            print(f\"Company Name: {company_name.strip()}\")\n",
    "            print(f\"Required Skills: {skills.strip()}\")\n",
    "            print(f\"More Info: {more_info}\")\n",
    "    \n",
    "            print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eaa3af-7393-4f52-8803-20067aaf3e5e",
   "metadata": {},
   "source": [
    "## Coleta de Dados Periódica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c64ef5-b0d5-47ed-810b-12d40b2b74c6",
   "metadata": {},
   "source": [
    "Como temos a base pronta, vamos fazer com que o programa realize coletas de dados periodicas, ou seja, ele realize essa extração em um determinado periodo de tempo. Nesse caso vamos fazer para que ocorrar a cada 10 minutos.  \n",
    "    <small>**OBS.:** Vou copiar o código anterior e aplicar as mudanças.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b18e8-62c7-4a8f-b31e-84d058c4b33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Put some skill that you are not familiar with\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> django\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out django\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edryck\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.timesjobs.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\Edryck\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.timesjobs.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 10 minutes...\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "print('Put some skill that you are not familiar with')\n",
    "# Variável para  o input usada no filtro\n",
    "unfamiliar_skill = input('>')\n",
    "print(f'Filtering out {unfamiliar_skill}')\n",
    "\n",
    "# Faz mais sentido criar uma função, assim ela será chamada em periodicamente\n",
    "def find_jobs():\n",
    "    # Para usar o get, iremos passar a URL do site que vamos realizar a extração\n",
    "    html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=e', verify=False).text\n",
    "    # Criando uma instância do BS\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    # Se utilizar o Inspencionar na página para localizar a parte que contém o conteúdo que queremos\n",
    "    jobs = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')\n",
    "    for job in jobs:\n",
    "        # Agora selecionando as vagas com a data de publicação especifica\n",
    "        # O texto dela vai estar dentro de um outro span\n",
    "        published_date = job.find('span', class_='sim-posted').span.text \n",
    "        if 'few' in published_date:\n",
    "            # Pegando agora o nome da empresa, na aula, a saída imprimindo apenas o texto, ele vem com alguns espaços\n",
    "            # Então será utilizado o replace() para substituir os espaços por nada\n",
    "            company_name = job.find('h3', class_='joblist-comp-name').text.replace(' ', '')\n",
    "            # Pegando as skills que a vaga pede\n",
    "            skills = job.find('span', class_='srp_skills').text.replace(' ', '')\n",
    "            # O conteúdo onde está o link para mais informações da vaga\n",
    "            more_info = job.header.h2.a['href']\n",
    "            # Filtro\n",
    "            if unfamiliar_skill not in skills:    \n",
    "                # Retirando o print com aspas triplas e substituindo pelas aspas duplas\n",
    "                # Vamos remover também os espaços em branco usando strip()\n",
    "                print(f\"Company Name: {company_name.strip()}\")\n",
    "                print(f\"Required Skills: {skills.strip()}\")\n",
    "                print(f\"More Info: {more_info}\")\n",
    "        \n",
    "                print('')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    while True:\n",
    "        # Chama a função\n",
    "        find_jobs()\n",
    "        # Agora vem a parte de chamada periodica, vamos utilizar o sleep\n",
    "        # Ele vai permitir que o programa espere por um determinado periodo de tempo\n",
    "        # time.sleep(600)\n",
    "        # ou\n",
    "        time_wait = 10\n",
    "        print(f'Waiting {time_wait} minutes...')\n",
    "        time.sleep(time_wait * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38518e-f18e-4fb4-aacc-04a059852c44",
   "metadata": {},
   "source": [
    "## Armazenando em Arquivos de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53606061-bc79-4164-b879-5b423a40690f",
   "metadata": {},
   "source": [
    "Agora vamos pegar essas informações e armazenar em um local separado. Eles serão armazenados em um diretórios chamado posts.  \n",
    "<small>**OBS.:** Vou copiar o código anterior e aplicar as mudanças.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8afd4c3-3cc7-4e1b-8a35-4e62d6a09f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "print('Put some skill that you are not familiar with')\n",
    "# Variável para  o input usada no filtro\n",
    "unfamiliar_skill = input('>')\n",
    "print(f'Filtering out {unfamiliar_skill}')\n",
    "\n",
    "# Faz mais sentido criar uma função, assim ela será chamada em periodicamente\n",
    "def find_jobs():\n",
    "    # Para usar o get, iremos passar a URL do site que vamos realizar a extração\n",
    "    html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=e', verify=False).text\n",
    "    # Criando uma instância do BS\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    # Se utilizar o Inspencionar na página para localizar a parte que contém o conteúdo que queremos\n",
    "    jobs = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')\n",
    "    for index, job in enumerate(jobs):\n",
    "        # Agora selecionando as vagas com a data de publicação especifica\n",
    "        # O texto dela vai estar dentro de um outro span\n",
    "        published_date = job.find('span', class_='sim-posted').span.text \n",
    "        if 'few' in published_date:\n",
    "            # Pegando agora o nome da empresa, na aula, a saída imprimindo apenas o texto, ele vem com alguns espaços\n",
    "            # Então será utilizado o replace() para substituir os espaços por nada\n",
    "            company_name = job.find('h3', class_='joblist-comp-name').text.replace(' ', '')\n",
    "            # Pegando as skills que a vaga pede\n",
    "            skills = job.find('span', class_='srp_skills').text.replace(' ', '')\n",
    "            # O conteúdo onde está o link para mais informações da vaga\n",
    "            more_info = job.header.h2.a['href']\n",
    "            # Filtro\n",
    "            if unfamiliar_skill not in skills:\n",
    "                with open('posts/{index}.txt', 'w') as f:\n",
    "                    # Retirando o print com aspas triplas e substituindo pelas aspas duplas\n",
    "                    # Vamos remover também os espaços em branco usando strip()\n",
    "                    f.write(f\"Company Name: {company_name.strip()} \\n\")\n",
    "                    f.write(f\"Required Skills: {skills.strip()} \\n\")\n",
    "                    f.write(f\"More Info: {more_info}\")\n",
    "                print(f'File saved: {index}.txt')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    while True:\n",
    "        # Chama a função\n",
    "        find_jobs()\n",
    "        # Agora vem a parte de chamada periodica, vamos utilizar o sleep\n",
    "        # Ele vai permitir que o programa espere por um determinado periodo de tempo\n",
    "        # time.sleep(600)\n",
    "        # ou\n",
    "        time_wait = 10\n",
    "        print(f'Waiting {time_wait} minutes...')\n",
    "        time.sleep(time_wait * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
